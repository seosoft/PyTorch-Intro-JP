{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 数字認識タスク\nNotebook の利用が初めての場合 - ようこそ！Notebook はコードの近くに結果が表示されるのを見ながら実行できる素晴らしいツールです。\n1. セルを実行するには、セルをクリックして SHIFT-Enter を押します。\n2. 何かが実行されると、変数はメモリに保存されます - それらを調べてください。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 訳注 Jupyter Notebook の使い方:\nこのワークショップでは、Azure Notebooks Service によってホストされている Jupyter ノートブックを使用します。あなたが Jupyter に精通しているならば、このセルを飛ばしてください。そうでない場合は、ここでいくつかのヒントがあります。\n\n* Python 3.6 のカーネルを選択してください。  \n* Jupyter には「セル」があります。ノートブックのどこかをクリックすると、ポインタを含むセルが選択されます。  \n* セル内のコードを実行または実行するには、トップメニューで [セル]-[セルの実行] を選択します。または、Shift + Enterキーを使用することもできます。  \n* セルを編集するには - セルをダブルクリックします。  \n* セルを実行すると、セルの左側の外側にあるポインタが [ ] から [ \\* ] に変わります。セルの実行が終了すると、[ \\* ] が数字に変わります。  \n* ノートブックまたはセルがハング(フリーズ)した場合は、トップメニューからノートブック全体の [Kernel]-[Restart Kernel and Clear All Outputs] を選択するか、そのセルだけを選択して [Kernel]-[Interrupt kernel] を選択します。再起動には数秒かかり、その後ですべてのセルを再実行する必要があります。  \n\nJupyterでは、順番に実行する必要はありません。すでに実行された前のセルに戻って再度実行することができます。特定のセルのコードが、前後のセルと依存関係を持たない限り、問題ありません。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 始めましょう\n最初のセルは必要なライブラリをインポートします。ここから始めましょう。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nimport numpy as np\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.onnx as onnx\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## データ!\nデータがなければ、機械学習では何もできません。今回は「**28x28のベクトルで数字を予測できるか？**」という課題を与えられています。\n  \n用意できたら、データを見てみる必要があります。次のセルは数字を視覚化するヘルパー関数です（健全性チェック）。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def draw_digits(digits):\n    fig, axes = plt.subplots(6, 20, figsize=(18, 7),\n                            subplot_kw={'xticks':[], 'yticks':[]},\n                            gridspec_kw=dict(hspace=0.1, wspace=0.1))\n    for i, ax in enumerate(axes.flat):\n        X, y = digits[i]\n        ax.imshow(255 - X.reshape(28,28) * 255, cmap='gray')\n        ax.set_title('{:.0f}'.format(torch.argmax(y).item()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "次のセルは、標準的な数値データセット（MNISTと呼ばれる）をダウンロードします。この呼び出しの transform 部分と target_transform 部分は、これから行うモデルにより適したデータにするための変換手順がいくつか入っています。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "digits = datasets.MNIST('data', train=True, download=True,\n                        transform=transforms.Compose([\n                            transforms.ToTensor(),\n                            transforms.Lambda(lambda x: x.reshape(28*28))\n                        ]),\n                        target_transform=transforms.Compose([\n                            transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, y, 1))\n                        ])\n                     )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "ここでデータの健全性をチェックします。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "draw_digits(digits)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "xやyを表示して、ざっとデータを見てください。これらは本当に単なる数字です！それらを見てみるといくつか奇妙に思えるかもしれません：\n\n1.「画像は 0～255 の 784個のベクトルではないのですか？」と思いましたか。データは見た通りです！新しい範囲が 0～1 になるように255で割ってベクトルを正規化しただけです（数値がより正確になるようにしました）  \n2.「y が数値の答えだったのではないですか？代わりに 10次元のベクトルになっています」と思いましたか。その通りです！これは答えの One-Hot 表現と呼ばれます。正解のインデクスの位置に \"1\" が入っています。繰り返しになりますが、これにより、これから作成するモデルで数値演算がよりよく機能するようになります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x, y = digits[0]\nprint(x)\nprint(y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# モデルを選択する\nいくつかのデータが得られたので、今度はうまくいくと思われるモデルの選択を開始します。これがデータサイエンスの科学的な部分が出てくるところです：私たちは推測し、それから仮定が正しいかどうかチェックします。 784個の蛇口から10本の異なるホースに水を分配する必要がある水道管のようなモデルを想像してみてください。これらの784個の蛇口は数字の個々のピクセルを表し、最後の10個のホースは実際の数（または少なくとも水が最も多く出ているもののインデックス）を表します。私たちの仕事は今その間の配管を選ぶことです。\n\n次の3つのセルは、順に難しくなる 3つの別の構造を表しています。\n\n1. 最初のモデルは単純な線形モデルです\n2. 2番目のモデルは3層ニューラルネットワークです\n3. 最後のモデルは全てが畳み込み層で構成されるニューラルネットワークです\n\nそれらがどのように働くかを完全に説明することはこのチュートリアルの範囲外ですが、正しいインデックスから最も多くの水を押し出せるように、最後に正しい水圧を生み出すように調整されなければならない内部ノブで配管していると想像してください 。下のセルほど、配管とそれに対応する内部ノブがより複雑になります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class SimpleLinear(nn.Module):\n    def __init__(self):\n        super(SimpleLinear, self).__init__()\n        self.layer1 = nn.Linear(28*28, 10)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        return F.softmax(x, dim=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class NeuralNework(nn.Module):\n    def __init__(self):\n        super(NeuralNework, self).__init__()\n        self.layer1 = nn.Linear(28*28, 512)\n        self.layer2 = nn.Linear(512, 512)\n        self.output = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = self.output(x)\n        return F.softmax(x, dim=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 1, 28, 28)\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# モデルパラメータの最適化\nいくつかのモデルができたので、数字を認識するのに良い仕事ができるかどうか確認するために内部パラメータを最適化する時が来ました！訓練する方法を調整するために最適化アルゴリズムを与える際に使用する、いくつかのパラメータです - これらはハイパーパラメータと呼ばれます。次に示す2つの変数です。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "learning_rate = 1e-3\nbatch_size = 64\nepochs = 5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "'learning_rate' は基本的に、アルゴリズムがモデルパラメータを学習する速度を指定します。あなたが「それを5000万に設定するのはどうかな？」と考えたとします。それが悪い考えである理由の最も良い例えはゴルフです。私はゴルフがうまくないので、本当によくわかりませんが、ショットを沈めようとしているのに毎回同じ距離ボールを打つようなものです。簡単ですね？今いる場所から穴までの正確な距離を打てばいいのです！それで完了です！さて、あなたは穴がどこにあるのかわからないが、おおざっぱな方向は知っているとします。あなたが選ぶ距離は実際に重要です。距離が長すぎると穴を逃すことになり、それを叩くと再び行きすぎます。距離が短すぎると、穴にたどり着くまでに時間がかかりますが、最終的には穴に到達するはずです。基本的には、1ショットあたりの正しい距離を推測してから試してみる必要があります。それが、穴に入れるための正しいパラメーターを見つけられるように、基本的に学習率がすることです。（さて、ゴルフの話は終わりです）\n\n\n以下は、完全に動作するのに必要な 3つのものです:\n1. **モデル** - 私たちが作ろうとしている関数であり、正解の数値を返すべきものです\n2. **コスト関数** (損失関数と呼ばれることもあります) - ゴルフの話は終わったと約束したことを覚えていますが、嘘をつきました。おかしなゴルフゲームであなたが穴の大体の方向を知っていると言ったのを覚えていますか？ コスト関数は穴までの距離を教えてくれます。ゼロになると穴に入ったことになるのです！実際の科学的な用語では、コスト関数は、モデルが正しい答えを得るのにどれほど悪いかを示すものです。ゴルフショットと同じように、コスト関数が減少するのも見るべきです。減少しなければ、何か間違っています。そうならば、ショットの距離（または学習率）をもっと小さい値に変更して、もう一度試してみます。それでもうまくいかない場合は、モデルを変更してください。\n3. **オプティマイザー** - この部分は、実際にモデルパラメーターを変更する部分です。それは我々がボールを打つべき方向の意味を持ち、正しい数字を予測する最良の内部の蛇口を見つけるために、モデル内部のすべての値を更新します。ここでは、バイナリークロスエントロピーコスト関数を使用します。これがうまく機能することがわかっているからです。さまざまなシナリオに合わせて、さまざまなコスト関数を選択できます。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 実行する場所\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} device'.format(device))\n\n# モデルの選択\nmodel = SimpleLinear().to(device)\n#model = NeuralNework().to(device)\n#model = CNN().to(device)\nprint(model)\n\n# 最良のパラメーターを決定するのに使用するコスト関数\ncost = torch.nn.BCELoss()\n\n# 最適なパラメーターを作成するために使用\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "'CUDA' について言及するのをすっかり忘れていました。基本的に for ループの中はすべての大きな行列の乗算問題であるため、GPUはこのプロセス全体をスピードアップします。PyTorchは基本的にどこで実行するか（CPU または CUDA - CUDA は計算を GPU に移行するためのプラットフォームです）をモデルに伝えるだけでいいのです。素晴らしい。\n\nそれでは、学習に進みます！データローダーの仕事は、データセット全体（ここでは 60,000個の例とそれに対応するラベル）を反復処理することですが、処理するデータを batch_sizeの サイズごとに取得します。batch_size は選択する必要があるもう1つのハイパーパラメータです。'epoch' は、データセット全体をループする回数です（これもまた、実験の進行状況に基づいて選択したものです）。\n\n次のセルの実行には時間がかかります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# loader\ndataloader = DataLoader(digits, batch_size=batch_size, num_workers=0, pin_memory=True)\n\n# golfing time!\nfor t in range(epochs):\n    print('Epoch {}'.format(t))\n    print('-------------------------------')\n    # go through the entire dataset once\n    for batch, (X, Y) in enumerate(dataloader):\n        X, Y = X.to(device), Y.to(device)\n        # zero out gradient\n        optimizer.zero_grad()\n        # make a prediction on this batch!\n        pred = model(X)\n        # how bad is it?\n        loss = cost(pred, Y)\n        # compute gradients\n        loss.backward()\n        # update parameters\n        optimizer.step()\n        \n        if batch % 100 == 0:\n            print('loss: {:>10f}  [{:>5d}/{:>5d}]'.format(loss.item(), batch * len(X), len(dataloader.dataset)))\n            \n    print('loss: {:>10f}  [{:>5d}/{:>5d}]\\n'.format(loss.item(), len(dataloader.dataset), len(dataloader.dataset)))\n    \nprint('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "さて、ここでは実際には何をしたのでしょうか。素晴らしい質問です。最初の実行した SimpleLinear モデルから説明します。基本的に1つの行列と1つのベクトルを学習しています。それだけです。実際にはこういうことです:\n\n\\begin{align}\nprediction = W \\cdot x + b\n\\end{align}\n\n\\begin{align}\n\\begin{bmatrix}\n\\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_{10}\n\\end{bmatrix}_{10 \\times 1} = \n\\begin{bmatrix}\nw_{1,1} &  \\ldots &  w_{1,784} \\\\\n\\vdots    & \\ddots &  \\\\\nw_{10,1}  & \\ldots & w_{10, 784} \\\\\n\\end{bmatrix}_{10 \\times 784} \n\\cdot\n\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{783} \\\\ x_{784}\\end{bmatrix}_{784 \\times 1}\n + \n\\begin{bmatrix}\nb_1 \\\\ \\vdots \\\\ b_{10}\n\\end{bmatrix}_{10 \\times 1}\n\\end{align}\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "行列 W とベクトル b は、SimpleLinear モデルが学習するものです。出力は 10×1 行列で、最大値を持っているものが予測したい数値のインデックスです。アルゴリズムが 2つの変数（およびサイズ）について学習した内容を見てみましょう:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for p in model.parameters():\n    print(p.shape)\n    print(p)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 動作しているか？\n正常に機能しているかどうかを判断する最良の方法は、学習プロセスで使用されていないデータでモデルをテストすることです。幸いなことに、そのようなデータセットがあります（これは既に使用したデータとは分けておいたものです）。以前と同じ方法でそれをすべてロードし、前のデータとは異なることを示すために表示してみます。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_digits = datasets.MNIST('data', train=False, download=True,\n                        transform=transforms.Compose([\n                            transforms.ToTensor(),\n                            transforms.Lambda(lambda x: x.reshape(28*28))\n                        ]),\n                        target_transform=transforms.Compose([\n                            transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, y, 1))\n                        ])\n                     )\ndraw_digits(test_digits)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "最初の数値でテストしましょう（7 です）。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x, y = test_digits[0]\nx = x.to(device).view(1, 28*28)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "評価に使用するモデル（推論とも呼ばれます）を指定して、これまでに使ったことのない数値を渡します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.eval()\nwith torch.no_grad():\n    pred = model(x)\n    pred = pred.to('cpu').detach()[0]\n    \nprint(pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "予測された数字が実際の数字と一致するかどうか見てみましょう:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred.argmax(0), y.argmax(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**すべて** のテストデータに対してどれだけうまくいっているか見てみましょう！"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 推論用の数値のデータローダー\ntest_dataloader = DataLoader(test_digits, batch_size=64, num_workers=0, pin_memory=True)\n\n# モデルを評価モードに設定\nmodel.eval()\ntest_loss = 0\ncorrect = 0\n\n# ループ\nwith torch.no_grad():\n    for batch, (X, Y) in enumerate(dataloader):\n        X, Y = X.to(device), Y.to(device)\n        pred = model(X)\n\n        test_loss += cost(pred, Y).item()\n        correct += (pred.argmax(1) == Y.argmax(1)).type(torch.float).sum().item()\n\ntest_loss /= len(dataloader.dataset)\ncorrect /= len(dataloader.dataset)\nprint('\\nTest Error:')\nprint('acc: {:>0.1f}%, avg loss: {:>8f}'.format(100*correct, test_loss))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# モデルの保存\n今回は PyTorch を使っていますが、すべてのフレームワークは異なります。モデルを (PyTorch の）内部フォーマットと ONNXフォーマットの両方で保存します（これらは大きな行列Wとベクトルbであることを覚えています）。モデルは数字を認識する必要があるたびに、実行されるプログラムにアセットとしてロードすることができます。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# グラフ全体を通すためのダミーの変数を作成\nx = torch.randint(255, (1, 28*28), dtype=torch.float).to(device) / 255\nonnx.export(model, x, 'model.onnx')\nprint('Saved onnx model to model.onnx')\n\n# PyTorch モデルを保存\ntorch.save(model.state_dict(), 'model.pth')\nprint('Saved PyTorch Model to model.pth')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 試してみましょう！\nこれでプロセス全体が完了したので、上に戻って試してみてください。次のように変更してみましょう:\n1. 他のモデル（SimpleLinear を NeuralNetwork, CNN）を選択。他のモデルは、画像が最終的な答えを得るために通過する追加の行列（Wとb）を学習することを除けば、呼び出し方はほとんど同じです。\n2. learning_rate、batch_size、epochなどのハイパーパラメーター。 変更で結果が良くなったり悪くなったりしますか？ 92％は問題ないですが、他のモデルとハイパーパラメータの組み合わせのほうがいいでしょうか。\n\n## 最後に\nフィードバックを歓迎します！これは役に立ちましたか？ 分かりにくい部分はありますか？簡単にでもいいので教えてください。"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}